{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8nCtvkCJL44IEqtU5yK4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthireddyrekula/Image_classification_tf/blob/main/Text_Classification_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to ensure colab uses only TensorFlow 2.x:"
      ],
      "metadata": {
        "id": "3gF89LSYatAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try: \n",
        "  %tensorflow_version 2.x \n",
        "except Exception: \n",
        "  pass"
      ],
      "metadata": {
        "id": "sHNyWlGJahaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary library including TensorFlow and Keras"
      ],
      "metadata": {
        "id": "pzjbEvmea5wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re \n",
        "from matplotlib import pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "vYfAuy6gbEDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kw7J2Lv_lT-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the train tweets"
      ],
      "metadata": {
        "id": "xxtk21OOb2Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('/content/drive/My Drive/train_tweets.csv')\n"
      ],
      "metadata": {
        "id": "bwIVD5b-b09t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperate the tweet text and the labels using the following code snippet:"
      ],
      "metadata": {
        "id": "6UGai-jhmNS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tweets.iloc[:, 2].values\n",
        "y = tweets.iloc[:, 1].values\n"
      ],
      "metadata": {
        "id": "asVLL_kXmXzO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "kPE3ryzTlv84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_corpus(text):\n",
        "    corpus = []\n",
        "    for i in range(len(text)):\n",
        "        tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\",\" \", str(text[i]))\n",
        "        tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\",\" \", tweet)\n",
        "        tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\",\" \", tweet)\n",
        "        tweet = tweet.lower()\n",
        "        tweet = re.sub(r\"can't\",\"can not\", tweet)\n",
        "        tweet = re.sub(r\"hv\",\"have\", tweet)\n",
        "        tweet = re.sub(r\"ur\",\"your\", tweet)\n",
        "        tweet = re.sub(r\"ain't\",\"is not\", tweet)\n",
        "        tweet = re.sub(r\"don't\",\"do not\", tweet)\n",
        "        tweet = re.sub(r\"couldn't\",\"could not\", tweet)\n",
        "        tweet = re.sub(r\"shouldn't\",\"should not\", tweet )\n",
        "        tweet = re.sub(r\"won't\",\"will not\", tweet)\n",
        "        tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "        tweet = re.sub(r\"it's\",\"it is\", tweet)\n",
        "        tweet = re.sub(r\"that's\",\"that is\", tweet)\n",
        "        tweet = re.sub(r\"where's\",\"where is\", tweet)\n",
        "        tweet = re.sub(r\"who's\",\"who is\", tweet)\n",
        "        tweet = re.sub(r\"\\W\",\" \", tweet)\n",
        "        tweet = re.sub(r\"\\d\",\" \", tweet)\n",
        "        tweet = re.sub(r\"[ðâï¼½³ªãºæååçæåä¹µó¾_ëìêè]\",\" \", tweet)\n",
        "        tweet =re.sub(r\"\\s[a-z]\\s\",\" \", tweet)\n",
        "        tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \", tweet)\n",
        "        tweet = re.sub(r\"^[a-z]\\s\",\" \", tweet)\n",
        "        tweet = re.sub(r\"^[a-z]\\s+\",\" \", tweet)\n",
        "        tweet = re.sub(r\"\\s+\",\" \", tweet)\n",
        "        tweet = re.sub(r\"^\\s\",\"\", tweet)\n",
        "        tweet = re.sub(r\"\\s$\",\"\", tweet)\n",
        "        corpus.append(tweet)\n",
        "        \n",
        "    #return the corpus\n",
        "    return corpus\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "2D61pNfll1Ah"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the text to feed to the model"
      ],
      "metadata": {
        "id": "NK6OFyaCnJ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check how many individual words present in the corpus\n",
        "corpus = clean_corpus(X)\n",
        "word_dict = {}\n",
        "for doc in corpus:\n",
        "    words = nltk.word_tokenize(doc)\n",
        "    for word in words:\n",
        "        if word not in word_dict:\n",
        "            word_dict[word] = 1\n",
        "        else:\n",
        "            word_dict[word] += 1\n",
        "            \n",
        "print(len(word_dict))\n",
        "\n",
        "\n",
        "#tokenising the texts\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "corpus_tokens = tokenizer.texts_to_sequences(corpus)"
      ],
      "metadata": {
        "id": "xDatnM1unObs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding text sequences"
      ],
      "metadata": {
        "id": "oTC_uIDvpLZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the average words present per comment\n",
        "print(corpus[0])\n",
        "print(corpus_tokens[0:2])\n",
        "\n",
        "num_of_words_in_doc =[]\n",
        "for doc in corpus_tokens:\n",
        "    num_of_words_in_doc.append(len(doc))\n",
        "print(num_of_words_in_doc)\n",
        "print(\"Average number of words: \", np.average(num_of_words_in_doc))\n",
        "\n",
        "\n",
        "\n",
        "# Padding the sequences\n",
        "corpus_pad = keras.preprocessing.sequence.pad_sequences(corpus_tokens,maxlen=25,padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "SYYqyonipBcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Validation Set\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(corpus_pad,y,test_size=0.2,random_state=101)\n"
      ],
      "metadata": {
        "id": "iI2CGQN3prke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building & Compiling the model\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = 25\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(input_dim=vocab_size,output_dim=50,input_length=max_length))\n",
        "model.add(keras.layers.LSTM(units=50,dropout=0.2,recurrent_dropout=0.2))\n",
        "model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "MoVYgfGup0GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "model.fit(X_train,y_train,batch_size=10,epochs=2, verbose=2)"
      ],
      "metadata": {
        "id": "GoWXh-RKpz0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "#Loading the test data\n",
        "test_tweets = pd.read_csv(\"/content/drive/My Drive/test_tweets_with_hate1.csv\")\n",
        "print(test_tweets.shape)\n",
        "\n",
        "#cleaning the text\n",
        "test_data = test_tweets['tweet']\n",
        "clean_test_data  = clean_corpus(test_data)\n",
        "\n",
        "#text to sequence and padding\n",
        "clean_test_data_token = tokenizer.texts_to_sequences(clean_test_data)\n",
        "clean_test_data_pad = pad_sequences(clean_test_data_token,maxlen=25,padding='post')"
      ],
      "metadata": {
        "id": "WPPFXk3ap7EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the submission file    '\n",
        "#final_prediction=model.predict(clean_test_data_pad) \n",
        "final_prediction = (model.predict(clean_test_data_pad) > 0.5).astype(\"int32\")\n",
        "#final_prediction=np.argmax(prediction)\n",
        "#final_prediction = model.predict_classes(clean_test_data_pad)\n",
        "\n",
        "test_tweets['label'] = final_prediction\n",
        "test_predictions = test_tweets[['id','label']]\n",
        "test_predictions.to_csv('LSTM3.csv',index=False)"
      ],
      "metadata": {
        "id": "nq8tkvBgWHyR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}